annotated-doc==0.0.4
annotated-types==0.7.0
anyio==4.12.0
asyncpg==0.31.0
bcrypt==5.0.0
click==8.3.1
dnspython==2.8.0
email-validator==2.3.0
fastapi==0.127.0
greenlet==3.3.0
h11==0.16.0
idna==3.11
pydantic==2.12.5
pydantic_core==2.41.5
python-dotenv==1.2.1
SQLAlchemy==2.0.45
starlette==0.50.0
typing-inspection==0.4.2
typing_extensions==4.15.0
uvicorn==0.40.0
# LangChain Integration - REMOVED (too much dependency bloat with Python 3.13)
# Using direct Ollama API calls and ChromaDB instead
packaging==24.1
# Direct dependencies for Ollama + ChromaDB
chromadb==0.5.11
requests==2.31.0
PyPDF2==3.0.1
# CPU-Optimized RAG Stack (2025 Lightweight)
# No GPU/CUDA - Pure CPU performance
redis==5.2.0
rank-bm25==0.2.2
# CPU-friendly embeddings (using just transformers, not sentence-transformers which requires torch)
# sentence-transformers==3.0.0  # REMOVED - requires PyTorch!
# transformers==4.42.4  # REMOVED - chromadb will handle this
# CPU-only vector operations (latest stable version)
faiss-cpu==1.13.2
# Efficient CPU computation (latest stable versions)
numpy==1.26.4
scipy==1.14.0
scikit-learn==1.5.1
# Optional: For better CPU parallelization
joblib==1.4.2
# Note: PyTorch (torch) is NOT needed when using Ollama
# Ollama handles all LLM inference separately
# sentence-transformers will auto-detect CPU and use it efficiently
